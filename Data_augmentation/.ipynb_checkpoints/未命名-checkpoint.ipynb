{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/icahuang/anaconda3/envs/st/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './data/original/0.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = './data/original'\n",
    "output_path = './data/augmented'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_tf(img_dir_path):\n",
    "    img_name_list = os.listdir(img_dir_path)\n",
    "    img_path_list = []\n",
    "    for i in range(len(img_name_list)):\n",
    "        img_path_list.append(os.path.join(img_dir_path, img_name_list[i]))\n",
    "    print(\"img_path_list:\")\n",
    "    print(img_path_list)\n",
    "\n",
    "\n",
    "    img = np.array(Image.open(img_path_list[0]))\n",
    "    augmented_img_list = np.zeros(img.shape)\n",
    "    print(augmented_img_list.shape)\n",
    "\n",
    "\n",
    "    for i in range(len(img_path_list)):\n",
    "        with tf.Session() as sess:\n",
    "            raw_img = tf.gfile.Open(img_path_list[i], 'rb').read()  #bytes, 把文件存储到string中返回\n",
    "            img_data = tf.image.decode_image(raw_img)    #convert the input bytes string into a “Tensor of type dtype.\n",
    "            #img_data = tf.image.convert_image_dtype(img_data, tf.float32)\n",
    "            img_data = tf.image.convert_image_dtype(img_data, tf.uint8)\n",
    "            cv2.imshow('raw_image', img_data.eval())     # 转为ndarray类型\n",
    "            print('raw_shape:', img_data.eval().shape)\n",
    "\n",
    "            \"\"\"resize\"\"\"\n",
    "            # img_data = tf.image.resize_images(img_data.eval(), (224, 224))\n",
    "\n",
    "            \"\"\"crop and black pad\"\"\"\n",
    "            # img_data = tf.image.resize_image_with_crop_or_pad(img_data, target_height=1000,\n",
    "            #                                                   target_width=1000)\n",
    "\n",
    "            \"\"\"按照倍数中心裁剪, 倍数=(0, 1]\"\"\"\n",
    "            # img_data = tf.image.central_crop(img_data, central_fraction=0.2)\n",
    "\n",
    "            \"\"\"pad \"\"\"\n",
    "            # img_data = tf.image.pad_to_bounding_box(img_data, offset_height=10, offset_width=10,\n",
    "            #                                         target_height=576+10, target_width=576+10)\n",
    "\n",
    "            \"\"\"crop\"\"\"\n",
    "            # img_data = tf.image.crop_to_bounding_box(img_data, 40, 40, 576-40, 576-40)\n",
    "\n",
    "            \"\"\"extract\n",
    "                                        o----->\n",
    "                                        |\n",
    "                                        |\n",
    "                                        v\n",
    "                                        x\n",
    "            \"\"\"\n",
    "            # img_data = tf.image.extract_glimpse(tf.expand_dims(img_data, 0), size=[100, 100],\n",
    "            #                                     offsets=tf.reshape(tf.constant([-.4, .4], dtype=tf.float32), [1, 2]))\n",
    "\n",
    "            \"\"\"roi pooling 必要操作 boxes为长宽比值!!! \"\"\"\n",
    "            # img_data = tf.image.crop_and_resize(tf.expand_dims(img_data, 0), boxes=[[0/576, 0/576, 1, 1]],\n",
    "            #                                     box_ind=[0], crop_size=[100, 100])\n",
    "\n",
    "            \"\"\"上下翻转/左右/转置翻转/90度旋转---(random_)flip_up_down/flip_left_right/transpose/rot90\"\"\"\n",
    "            img_data = tf.image.rot90(img_data, k=1)\n",
    "\n",
    "            \"\"\"Converting Between Colorspaces\"\"\"\n",
    "            \"\"\"灰度\"\"\"\n",
    "            # img_data = tf.image.rgb_to_grayscale(img_data)\n",
    "            \"\"\"图像亮度[-1, 1]\"\"\"\n",
    "            # img_data = tf.image.adjust_brightness(img_data, delta=-.7)\n",
    "            \"\"\"随机图像亮度\"\"\"\n",
    "    #         img_data = tf.image.random_brightness(img_data, max_delta=0.6)\n",
    "            \"\"\"随机对比度\"\"\"\n",
    "    #         img_data = tf.image.random_contrast(img_data, lower=0, upper=4)\n",
    "            \"\"\"随机色调\"\"\"\n",
    "            # img_data = tf.image.random_hue(img_data, 0.5)\n",
    "            \"\"\"随机饱和度\"\"\"\n",
    "    #         img_data = tf.image.random_saturation(img_data, lower=0, upper=2)\n",
    "            \"\"\"图片标准化    (x - mean) / max(stddev, 1.0/sqrt(image.NumElements()))\"\"\"\n",
    "            # img_data = tf.image.per_image_standardization(img_data)\n",
    "            \"\"\"draw boxes\"\"\"\n",
    "            # img_data = tf.image.draw_bounding_boxes(tf.expand_dims(img_data, 0), [[[0.1, 0.2, 0.5, 0.9]]])\n",
    "            print('Tensor:', img_data)\n",
    "            print(img_data.eval().shape)\n",
    "            print(img_data.eval())\n",
    "            \n",
    "            # #plt显示图像\n",
    "            # plt.figure(1)\n",
    "            # plt.imshow(img_data.eval())\n",
    "            # plt.show()\n",
    "\n",
    "            # #cv2显示图像\n",
    "            # cv2.imshow('changed', img_data.eval())\n",
    "            # cv2.waitKey()\n",
    "\n",
    "            augmented_img_list.append(img_data.eval())\n",
    "\n",
    "    return augmented_img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation_tf(img_dir_path):\n",
    "    img_name_list = os.listdir(img_dir_path)\n",
    "    #将每个图片的路径存储到img_path_list\n",
    "    img_path_list = []\n",
    "    for i in img_name_list:\n",
    "        img_path_list[i] = os.path.join(img_dir_path, img_name_list[i])\n",
    "    print(\"img_path_list:\")\n",
    "    print(img_path_list)\n",
    "\n",
    "    count = np.array(img_path_list).shape[0]\n",
    "\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_array(imgArr, output_path):\n",
    "    #检查存放输出图像的文件夹是否已经被创建，若没有则先创建\n",
    "    if os.path.isdir(output_path):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir(output_path)\n",
    "\n",
    "    i = 0\n",
    "    for imgFile in imgArr:\n",
    "        #scipy.misc.imsave(output_path + '/' +  '%s_%d.jpg' %(name, i), imgFile)\n",
    "        path = os.path.join(output_path, '%s_%d.jpg' % ('augmented', i))\n",
    "        scipy.misc.imsave(path, imgFile)\n",
    "        i = i + 1\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_path_list:\n",
      "['./data/original/48.jpg', './data/original/8.jpg', './data/original/9.jpg', './data/original/14.jpg', './data/original/28.jpg', './data/original/29.jpg', './data/original/15.jpg', './data/original/17.jpg', './data/original/16.jpg', './data/original/12.jpg', './data/original/13.jpg', './data/original/39.jpg', './data/original/11.jpg', './data/original/10.jpg', './data/original/38.jpg', './data/original/21.jpg', './data/original/35.jpg', './data/original/34.jpg', './data/original/20.jpg', './data/original/36.jpg', './data/original/22.jpg', './data/original/23.jpg', './data/original/37.jpg', './data/original/33.jpg', './data/original/27.jpg', './data/original/26.jpg', './data/original/32.jpg', './data/original/18.jpg', './data/original/24.jpg', './data/original/30.jpg', './data/original/31.jpg', './data/original/25.jpg', './data/original/19.jpg', './data/original/42.jpg', './data/original/4.jpg', './data/original/5.jpg', './data/original/43.jpg', './data/original/7.jpg', './data/original/41.jpg', './data/original/40.jpg', './data/original/6.jpg', './data/original/2.jpg', './data/original/44.jpg', './data/original/45.jpg', './data/original/3.jpg', './data/original/47.jpg', './data/original/1.jpg', './data/original/0.jpg', './data/original/46.jpg']\n",
      "(2500, 2500, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Operation 'Identity_2' has been marked as not fetchable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e4300586a766>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_augmentation_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-32-5964c5cd2aae>\u001b[0m in \u001b[0;36mdata_augmentation_tf\u001b[0;34m(img_dir_path)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;31m#img_data = tf.image.convert_image_dtype(img_data, tf.float32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mimg_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_image_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_image'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# 转为ndarray类型\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[0;32m--> 570\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_dup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[0;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4453\u001b[0m                        \u001b[0;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4454\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4455\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1103\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1105\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/st/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_assert_fetchable\u001b[0;34m(self, graph, op)\u001b[0m\n\u001b[1;32m    438\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fetchable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m       raise ValueError(\n\u001b[0;32m--> 440\u001b[0;31m           'Operation %r has been marked as not fetchable.' % op.name)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Operation 'Identity_2' has been marked as not fetchable."
     ]
    }
   ],
   "source": [
    "a = data_augmentation_tf(input_path)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:st] *",
   "language": "python",
   "name": "conda-env-st-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
